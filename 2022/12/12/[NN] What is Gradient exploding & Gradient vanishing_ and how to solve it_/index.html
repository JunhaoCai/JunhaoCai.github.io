<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    <meta name="description" content="Hexo Theme Redefine">
    <meta name="author" content="Jun-ho Chae">
    
    <title>
        
            (NN) What is Gradient exploding &amp; Gradient vanishing? and how to solve it? |
        
        Jun-ho Chae&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/c-solid.svg">
    
<link rel="stylesheet" href="/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/css/v5-font-face.min.css">

    
<link rel="stylesheet" href="/css/duotone.min.css">

    
<link rel="stylesheet" href="/css/brands.min.css">

    
<link rel="stylesheet" href="/css/solid.min.css">

    
<link rel="stylesheet" href="/css/css2.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.json"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/c-solid.svg","favicon":"/images/c-solid.svg","article_img_align":"center","right_side_width":"210px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"/images/light1111.jpg","dark":"/images/dark111.jpg"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"Welcome to my Blog. Have a nice day! üòä"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"0.3.5"};
    REDEFINE.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">
    
    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Jun-ho Chae&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               target="_blank" rel="noopener" href="https://blog.csdn.net/cjh0318"
                            >
                                LINKS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       target="_blank" rel="noopener" href="https://blog.csdn.net/cjh0318">LINKS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">
            <div class="article-title">
                <span class="title-hover-animation"><h1 style="font-size:2rem; font-weight: bold; margin: 10px 0;">(NN) What is Gradient exploding &amp; Gradient vanishing? and how to solve it?</h1></span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/c-solid.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Jun-ho Chae</span>
                            
                                <span class="author-label">lol</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-duotone fa-pen-line"></i>&nbsp;
        <span class="pc">2022-12-12 23:59:32</span>
        <span class="mobile">2022-12-12 23:59</span>
    </span>
    
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="Today-I-intend-to-discuss-gradient-explosion-and-vanishing-issues-üßê"><a href="#Today-I-intend-to-discuss-gradient-explosion-and-vanishing-issues-üßê" class="headerlink" title="Today I intend to discuss gradient explosion and vanishing issues. üßê"></a>Today I intend to discuss gradient explosion and vanishing issues. üßê</h1><h2 id="1-An-intuitive-understanding-of-what-gradient-explosion-and-gradient-disappearance-are-ü§î"><a href="#1-An-intuitive-understanding-of-what-gradient-explosion-and-gradient-disappearance-are-ü§î" class="headerlink" title="1. An intuitive understanding of what gradient explosion and gradient disappearance are. ü§î"></a>1. An intuitive understanding of what gradient explosion and gradient disappearance are. ü§î</h2><p>You and I know about when the person who does more things than yesterday and develops himself can get crazy successful. I want to organize this thing to map with math.<br><strong>what is the 0.99 to 100th power and 1.1 to 100th power?</strong><br><em><strong>(0.99 ** 100 &#x3D; 0.3660323413) And (1.1 ** 100 &#x3D; 13,780.6123398223)</strong></em> <strong>What a big difference!</strong> ü§† If you know anything about calculus, you know that if you multiply successively by a number <strong>greater than 1 you will get close to infinity (Gradient exploding)</strong>, and conversely multiplying by a number <strong>less than 1 you will get close to 0 (Gradient vanishing).</strong></p>
<h2 id="2-Why-the-neural-network-should-multiply-successively-by-a-number-Causes-gradient-vanishing-and-gradient-explosion-ü§®"><a href="#2-Why-the-neural-network-should-multiply-successively-by-a-number-Causes-gradient-vanishing-and-gradient-explosion-ü§®" class="headerlink" title="2. Why the neural network should multiply successively by a number? (Causes gradient vanishing and gradient explosion.ü§®)"></a>2. Why the neural network should multiply successively by a number? (Causes gradient vanishing and gradient explosion.ü§®)</h2><p>When we train a neural network, it involves chain rule and gradient descent (All of these require multiplication).üò≠ Usually, the neural network we are about to train will almost always contain nonlinear activation functions (sigmoid, tanh, etc.).<br><a class="link"   target="_blank" rel="noopener" href="https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4" >This ‚Äòsigmoid‚Äô graph is from here.<i class="fas fa-external-link-alt"></i></a><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/0251e6889f5c428eb88f8cadf25fe3e6.png"
                      alt="Âú®ËøôÈáåÊèíÂÖ•ÂõæÁâáÊèèËø∞"
                ><br><a class="link"   target="_blank" rel="noopener" href="https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4" >This ‚Äòtanh‚Äô graph is from here.<i class="fas fa-external-link-alt"></i></a><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/ecdad391a00a402782d6a9892c527606.png"
                      alt="Âú®ËøôÈáåÊèíÂÖ•ÂõæÁâáÊèèËø∞"
                ><br><a class="link"   target="_blank" rel="noopener" href="https://www.researchgate.net/figure/ReLU-activation-red-and-derivative-blue-for-efficient-gradient-computation_fig3_342435907" >This ‚ÄòReLu‚Äô graph is from here.<i class="fas fa-external-link-alt"></i></a><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/0f5d9569960f4fadb49de6948237b714.png"
                      alt="Âú®ËøôÈáåÊèíÂÖ•ÂõæÁâáÊèèËø∞"
                ><br>Based on the three graphs above, I can conclude that there is a risk of <strong>gradient vanishing</strong> if the <strong>sigmoid function and the tanh function</strong> are used. <strong>Because their values are less than 1, but the ReLu function is an exception.ü•≤</strong></p>
<h2 id="3-How-do-I-solve-the-gradient-explosion-ü§ï-Gradient-clipping"><a href="#3-How-do-I-solve-the-gradient-explosion-ü§ï-Gradient-clipping" class="headerlink" title="3. How do I solve the gradient explosion? ü§ï(Gradient clipping)"></a>3. How do I solve the gradient explosion? ü§ï(Gradient clipping)</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/58120a8700bd48be8038bd9dae444b5d.png"
                      alt="ËØ∑Ê∑ªÂä†ÂõæÁâáÊèèËø∞"
                ><br>I will choose to use gradient clipping. first, I will set a gradient threshold, suppose it is set to 15. after that, I need to check the gradient of weight, if this gradient is greater than 15, I will process it according to the formula in the picture above. In detail, I will divide the tensor of the current gradient by the norm of this tensor. According to this formula, we will get <strong>a tensor of size 1 and the same direction as before</strong>, and then multiply it by 15. The result is that we can <strong>constrain it to 15.</strong> Because <strong>the direction represents the direction of gradient descent and the norm represents the length of the step.</strong> ü§ó</p>
<h3 id="How-do-I-use-gradient-clipping-in-PyTorch-ü§†"><a href="#How-do-I-use-gradient-clipping-in-PyTorch-ü§†" class="headerlink" title="How do I use gradient clipping in PyTorch? ü§†"></a>How do I use gradient clipping in PyTorch? ü§†</h3><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(output, y)</span><br><span class="line">model.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p.grad.norm())</span><br><span class="line">    torch.nn.utils.clip_grad_norm(p, max_norm=<span class="number">10</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></div>
<h2 id="4-List-some-solutions-that-can-mitigate-gradient-explosion-and-gradient-vanishing-üßê"><a href="#4-List-some-solutions-that-can-mitigate-gradient-explosion-and-gradient-vanishing-üßê" class="headerlink" title="4. List some solutions that can mitigate gradient explosion and gradient vanishing. üßê"></a>4. List some solutions that can mitigate gradient explosion and gradient vanishing. üßê</h2><ol>
<li>Gradient clipping.</li>
<li>Weight decay.</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(),lr=<span class="number">0.05</span>,weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure></div>

<ol start="3">
<li>Batchnorm: The distribution of the input values of the neurons <strong>in the backpropagation</strong> is changed to <strong>a standard normal distribution</strong> with <strong>a mean of 0 and a variance of 1</strong>. This treatment leads to <strong>a large change in the loss function, which makes the gradient larger and avoids the gradient disappearance problem.</strong></li>
<li>Use activation functions such as ReLu, ELu, etc.</li>
<li>Shortcut</li>
<li>Some neural networks with ‚Äúgates‚Äù. (LSTM, GRU, etc.)</li>
<li>Appropriate control of the number of layers of the neural network.</li>
<li>And so on‚Ä¶</li>
</ol>
<h1 id="Complete-Code"><a href="#Complete-Code" class="headerlink" title="Complete Code"></a>Complete Code</h1><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;KMP_DUPLICATE_LIB_OK&quot;</span>]=<span class="string">&quot;TRUE&quot;</span></span><br><span class="line"></span><br><span class="line">num_time_steps = <span class="number">50</span></span><br><span class="line">input_size = <span class="number">1</span></span><br><span class="line">hidden_size = <span class="number">16</span></span><br><span class="line">output_size = <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>)-&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rnn = nn.RNN(</span><br><span class="line">            input_size=input_size,</span><br><span class="line">            hidden_size=hidden_size,</span><br><span class="line">            num_layers=<span class="number">1</span>,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self. rnn.parameters():</span><br><span class="line">            nn.init.normal_(p, mean=<span class="number">0.0</span>, std=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, hidden_prev</span>):</span><br><span class="line">        out, hidden_prev = self.rnn(x, hidden_prev)</span><br><span class="line">        <span class="comment"># [1, seq, h] =&gt; [seq, h]</span></span><br><span class="line">        out = out.view(-<span class="number">1</span>, hidden_size)</span><br><span class="line">        out = self.linear(out) <span class="comment"># [seq, h] =&gt; [seq, 1]</span></span><br><span class="line">        out = out.unsqueeze(dim=<span class="number">0</span>) <span class="comment"># =&gt; [1, seq, 1]</span></span><br><span class="line">        <span class="keyword">return</span> out, hidden_prev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr)</span><br><span class="line"></span><br><span class="line">hidden_prev = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6000</span>):</span><br><span class="line">    start = np.random.randint(<span class="number">3</span>, size=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    time_steps = np.linspace(start, start+<span class="number">10</span>, num_time_steps)</span><br><span class="line">    data = np.sin(time_steps)</span><br><span class="line">    data = data.reshape(num_time_steps, <span class="number">1</span>)</span><br><span class="line">    x = torch.tensor(data[:-<span class="number">1</span>]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps -<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    y = torch.tensor(data[<span class="number">1</span>:]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps -<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    output, hidden_prev = model(x, hidden_prev)</span><br><span class="line">    hidden_prev = hidden_prev.detach()</span><br><span class="line"></span><br><span class="line">    loss = criterion(output, y)</span><br><span class="line">    model.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p.grad.norm())</span><br><span class="line">        torch.nn.utils.clip_grad_norm(p, max_norm=<span class="number">10</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">iter</span> % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Iteration:<span class="subst">&#123;<span class="built_in">iter</span>&#125;</span>  loss<span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">start = np.random.randint(<span class="number">3</span>, size=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">time_steps = np.linspace(start, start + <span class="number">10</span>, num_time_steps)</span><br><span class="line">data = np.sin(time_steps)</span><br><span class="line">data = data.reshape(num_time_steps, <span class="number">1</span>)</span><br><span class="line">x = torch.tensor(data[:-<span class="number">1</span>]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = torch.tensor(data[<span class="number">1</span>:]).<span class="built_in">float</span>().view(<span class="number">1</span>, num_time_steps - <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">predictions = []</span><br><span class="line"><span class="built_in">input</span> = x[:, <span class="number">0</span>, :]</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    (pred, hidden_prev) = model(<span class="built_in">input</span>, hidden_prev)</span><br><span class="line">    <span class="built_in">input</span> = pred</span><br><span class="line">    predictions.append(pred.detach().numpy().ravel()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">x = x.data.numpy().ravel()</span><br><span class="line">y = y.data.numpy()</span><br><span class="line">plt.scatter(time_steps[:-<span class="number">1</span>], x.ravel(), s=<span class="number">90</span>)</span><br><span class="line">plt.plot(time_steps[:-<span class="number">1</span>], x.ravel())</span><br><span class="line"></span><br><span class="line">plt.scatter(time_steps[<span class="number">1</span>:], predictions)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></div>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/bbf59589e7e043788005cb4ec8f2eb5c.png"
                      alt="ËØ∑Ê∑ªÂä†ÂõæÁâáÊèèËø∞"
                ></p>
<h1 id="Finally-ü§©"><a href="#Finally-ü§©" class="headerlink" title="Finally ü§©"></a>Finally ü§©</h1><p>Thank you for the current age of knowledge sharing and the people willing to share it, thank you! The knowledge on this blog is what I‚Äôve learned on this <a class="link"   target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1YG411876u?p=1&vd_source=6abb86d30183a431fb6faa2208ed3c8b" >site<i class="fas fa-external-link-alt"></i></a>, thanks for the support!  üòá</p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>Post titleÔºö(NN) What is Gradient exploding &amp; Gradient vanishing? and how to solve it?</li>
        <li>Post authorÔºöJun-ho Chae</li>
        <li>Create timeÔºö2022-12-12 23:59:32</li>
        <li>
            Post linkÔºöhttps://redefine.evanluo.top/2022/12/12/[NN] What is Gradient exploding &amp; Gradient vanishing_ and how to solve it_/
        </li>
        <li>
            Copyright NoticeÔºöAll articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

                </div>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2022/12/13/%5BNLP%5D%20Description%20and%20implementation%20of%20LSTM%20neural%20network./"
                            >
                                <span class="left arrow-icon flex-center">
                                <i class="fas fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">(NLP) Description and implementation of LSTM neural network.</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2022/12/12/%5BNLP%5D%20Teach%20RNN%20to%20output%20%E2%80%98ihello%E2%80%98%20when%20%E2%80%98hihell%E2%80%98%20is%20entered./"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">(NLP) Teach RNN to output ‚Äòihello‚Äò when ‚Äòhihell‚Äò is entered.</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                <i class="fas fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            

        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div style="font-size: 1.3rem;margin-top: 0; margin-bottom: 0.8rem; transition-duration: 0.1s;"><i class="fa-solid fa-list"></i> <strong>Contents</strong></div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Today-I-intend-to-discuss-gradient-explosion-and-vanishing-issues-%F0%9F%A7%90"><span class="nav-text">Today I intend to discuss gradient explosion and vanishing issues. üßê</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-An-intuitive-understanding-of-what-gradient-explosion-and-gradient-disappearance-are-%F0%9F%A4%94"><span class="nav-text">1. An intuitive understanding of what gradient explosion and gradient disappearance are. ü§î</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Why-the-neural-network-should-multiply-successively-by-a-number-Causes-gradient-vanishing-and-gradient-explosion-%F0%9F%A4%A8"><span class="nav-text">2. Why the neural network should multiply successively by a number? (Causes gradient vanishing and gradient explosion.ü§®)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-How-do-I-solve-the-gradient-explosion-%F0%9F%A4%95-Gradient-clipping"><span class="nav-text">3. How do I solve the gradient explosion? ü§ï(Gradient clipping)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#How-do-I-use-gradient-clipping-in-PyTorch-%F0%9F%A4%A0"><span class="nav-text">How do I use gradient clipping in PyTorch? ü§†</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-List-some-solutions-that-can-mitigate-gradient-explosion-and-gradient-vanishing-%F0%9F%A7%90"><span class="nav-text">4. List some solutions that can mitigate gradient explosion and gradient vanishing. üßê</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Complete-Code"><span class="nav-text">Complete Code</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Finally-%F0%9F%A4%A9"><span class="nav-text">Finally ü§©</span></a></li></ol>
    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2023&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Jun-ho Chae</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v0.3.5</a>
        </div>
        
        
    </div>
    <link rel="stylesheet" href="//evan.beee.top/css/waline.css"/>
    <script src="//evan.beee.top/js/waline.js"></script>
    
<link rel="stylesheet" href="/css/regular.min.css">

</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fa-duotone fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-duotone fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>




    
<script src="/js/lazyload.js"></script>



<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>



</body>
</html>
