[{"title":"How to read a paper efficientlyÔºü","url":"/2022/12/12/How%20to%20read%20a%20paper%20efficiently_%EF%BC%88%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E7%9A%84%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%9F%EF%BC%89/","content":"1. The universal format of the paper.A paper will exist in the following structure. \n\ntitle\nAbstract\nIntro\nMethod\nexperiment\nconclusion\n\n2. Three steps to reading a paper.Pass 1The first step is to focus on whether this paper is worth your time to read.Specific reading orderÔºö 1 -&gt; 2 -&gt; 6 -&gt; 5(graphÔºåchart) -&gt; 4(graphÔºåchart)‚ö†Ô∏èÔºö Don‚Äôt pay attention to details.\nPass 2The second step, after the first step, has been judged that this paper is worth reading. So, this time, pay attention to the graphs and tables to determine exactly what the X and Y axes are for and what each point is for. Pay attention to how much the author‚Äôs method has increased compared to the previously available methods. \n\nLook mainly at the graphs and tables.\nCircle the cited documents.\n\nPass 3After the first and second readings, you know what the article is about in general. So, in the third step, know what each sentence and each paragraph are doing. Then, repeat in your head how to achieve this. Think: What would you do if you were yourself and make up the whole process? If the researcher were me, how could I continue to build on this? \n\nThanks to Mr. Li Mu for sharing, the video explanation is detailed in the following link üîó:Sharing URL\n\n"},{"title":"How can I reduce the memory processing large dataset(Array, Tensor, etc.) in PyTorch?","url":"/2022/12/12/How%20can%20I%20reduce%20the%20memory%20processing%20large%20dataset(Array,%20Tensor,%20etc.)%20in%20Pytorch_/","content":"The introduction: When I was processing the very large data set, I wanted to reduce that stored in my memory to run through my MacBook(üò≠), I know that is poor for deep learning.Start: When we process the data we will copy the tensor or list etc. üòä So let‚Äôs read the code detail of Python.b = 1c = 1before = id(b)b += cprint(b)print(id(b) == before)d = 1e = 1d = d + eprint(d)print(id(d) == before)\n\n\nIn that case, I thought you should have a huge ‚ÄòQuestion Mark‚Äô and ‚ÄúSo??? Is it a new land in the world?‚Äù üòä Let‚Äôs go next step. üèÉb = [1]c = [1]before = id(b)b += cprint(b)print(id(b) == before)d = [1]e = [1]d = d + eprint(d)print(id(d) == before)\n\nIncredible, Amazing!! ü§Ø What‚Äôs that? why do I add the [ ] around the 1 integer the result will be different? I want to answer you in the last chapter. üòÅ So Let‚Äôs go!\nimport torchb = torch.zeros(    (3, 4, 5))c = torch.ones(    (3, 4, 5))before = id(b)b += cprint(b)print(id(b) == before)d = torch.zeros(    (3, 4, 5))e = torch.ones(    (3, 4, 5))d = d + eprint(d)print(id(d) == before)\n\nLet‚Äôs reveal this mysterious veilÔºÅ1. The KEY WORD is what the Immutable element and mutable element are.\nThe immutable elements: Integers, floats, strings, tuples, etc.\nThe mutable elements: Lists, Dictionaries, Tensors, etc.\n\n2. What is the function of ‚Äú&#x3D;‚Äù?Binding an object reference to an object in memory.3. How does the ‚Äú&#x3D;‚Äù work in the changing of the immutable elements?Directly create a new integer value in memory and then bind the variable reference to it. So, The type of an immutable element, ‚ÄúA +&#x3D; B‚Äù is the same as ‚ÄúA &#x3D; A + B‚Äù.üòä.4. What about the mutable elements?As you see, using the ‚ÄúA +&#x3D; B‚Äù Python will add value to the same memory area. By contrast, using the ‚ÄúA &#x3D; A + B‚Äù Python creates a new variable area to the memory and computes the value then to be bound by the variable reference ‚ÄúA‚Äù. üëºConclusionI should get into the habit of considering the difference between ‚ÄúA+&#x3D;B‚Äù and ‚ÄúA &#x3D; A+ B‚Äù. So do you. üíó"},{"title":"(CV) How to Develop a CNN From Scratch for CIFAR-10 Photo Classification in PyTorch?","url":"/2022/12/12/%5BComputer%20Vision%5D%20How%20to%20Develop%20a%20CNN%20From%20Scratch%20for%20CIFAR-10%20Photo%20Classification%20in%20PyTorch_/","content":"0. Statement ü´°In this blog, the author, summarizing what he has learned, has built a CNN model to solve the CIFAR-10 photo classification problem (without using nonlinear activation functions) using DataLoader, SummaryWriter, torchvision, and torch.nn, torch.optim, etc. If there are any errors, please feel free to correct them. üè´\n1. Planning üßê\nEvaluating the dataset(shape, training set, testing set, etc).\nBuilding CIFAR 10 Neural Networks.\nSelecting loss functions and optimizers.\nConstructing the accuracy function.\nVisualization with tensorboard.\nAdjusting parameters according to the final effect.\n\n2. What is the CIFAR-10 Photo dataset? ü§îWhich is from here.\n2.1 The shape of the dataset. ü§óI have used torchvision to get the CIFAR10 dataset.ü§©\nimport torchvisionfrom torch.utils.data import DataLoaderdataset = torchvision.datasets.CIFAR10(&#x27;data&#x27;, train=True, transform=torchvision.transforms.ToTensor(),                                       download=True)dataloader = DataLoader(dataset, batch_size=64)for data in dataloader:    imgs, targets = data    print(imgs.shape)\nAs you can see, the batch_size is 64, there are 3 channels, the height is 32, and the width is also 32. I can be so sure about the meaning of each number because of the introduction of Shape in Conv2 on the PyTorch website. \n\n2.2 Training set &amp; Testing set. ü§† # datasettraining_data = torchvision.datasets.CIFAR10(&quot;root=data&quot;, train=True, transform=torchvision.transforms.ToTensor(),                                          download=True)testing_data = torchvision.datasets.CIFAR10(root=&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# lengthtraining_data_size = len(training_data)print(&quot;The length of training_data is&quot;,training_data_size)testing_data_size = len(testing_data)print(&quot;The length of testing_data is&quot;,testing_data_size)                            \n\n3. How do I build a CNN neural network?Since this blog is mainly about learning how to build CNNs with PyTorch, I followed along online to learn a neural network to build it, From here.\n\nimport torchfrom torch import nnclass Net(nn.Module):    def __init__(self):        super().__init__()        self.model = nn.Sequential(            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            nn.MaxPool2d(kernel_size=2),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            nn.MaxPool2d(kernel_size=2),            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            nn.MaxPool2d(2),            nn.Flatten(),            nn.Linear(in_features=64*4*4, out_features=64),            nn.Linear(in_features=64, out_features=10)        )    def forward(self, x):        x = self.model(x)        return x\n3. Selecting loss functions and optimizers. üòé# Loss_functionloss_fn = nn.CrossEntropyLoss()# Optimizeroptimizer = torch.optim.SGD(net.parameters(), lr=Learning_rate)\n4. Constructing the accuracy function. üòäaccuracy = (outputs.argmax(1) == targets).sum()total_accuracy = total_accuracy + accuracywriter.add_scalar(&quot;test_accuracy&quot;, total_accuracy/testing_data_size, total_testing_step)\n5. Visualization with tensorboard. ü§ì\nconda activate XXXX\ntensorboard ‚Äìlogdir&#x3D;&#x2F;AD&#x2F;DR&#x2F;ES&#x2F;S ‚Äìload_fast&#x3D;true\nTensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n\n6. Adjusting parameters according to the final effect.Because I am using a CPU to train this neural network, it takes too long. Please use GPU to learn if the condition allows. ü§ì\nmodel.py\nimport torchfrom torch import nn# Building Neural Networksclass Net(nn.Module):    def __init__(self):        super().__init__()        self.model = nn.Sequential(            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),            nn.MaxPool2d(kernel_size=2),            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),            nn.MaxPool2d(kernel_size=2),            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),            nn.MaxPool2d(2),            nn.Flatten(),            nn.Linear(in_features=64*4*4, out_features=64),            nn.Linear(in_features=64, out_features=10)        )    def forward(self, x):        x = self.model(x)        return x\ntrain.py\nimport torch.optimimport torchvisionfrom torch import nnfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom model import *import osos.environ[&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;] = &#x27;2&#x27;# Set some parameters of the training network# Record the number of training sessionstotal_training_step = 0# Record the number of teststotal_testing_step = 0# Number of rounds of trainingepoch = 10# 1e-2 = 1 * (10)^(-2) = 1 /100 = 0.01Learning_rate = 1e-2# Add tensorboardwriter = SummaryWriter(&quot;logs_train&quot;)# Preparing the data settraining_data = torchvision.datasets.CIFAR10(&quot;root=data&quot;, train=True, transform=torchvision.transforms.ToTensor(),                                          download=True)testing_data = torchvision.datasets.CIFAR10(root=&quot;data&quot;, train=False, transform=torchvision.transforms.ToTensor(),                                         download=True)# length training_data_size = len(training_data)print(&quot;The length of training_data is&quot;,training_data_size)testing_data_size = len(testing_data)print(&quot;The length of testing_data is&quot;,testing_data_size)# Using DataLoader to load datasetstraining_dataloader = DataLoader(training_data, batch_size=64)testing_dataloader = DataLoader(testing_data, batch_size=64)# Create Network Modelnet = Net()# Creating loss functionsloss_fn = nn.CrossEntropyLoss()# Optimizeroptimizer = torch.optim.SGD(net.parameters(), lr=Learning_rate)for i in range(epoch):    print(f&quot;-------The &#123;i+1&#125;th training round starts-------&quot;)    # Training steps begin    net.train()    for data in training_dataloader:        imgs, targets = data        outputs = net(imgs)        loss = loss_fn(outputs, targets)        # Optimizer optimization model        optimizer.zero_grad()        loss.backward()        optimizer.step()        total_training_step += 1        if total_training_step % 10 == 0:            print(f&quot;Number of training sessions: &#123;total_training_step&#125;, Loss: &#123;loss.item()&#125;&quot;)            writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_training_step)    # Test steps begin    net.eval()    total_testing_loss = 0    total_accuracy = 0    with torch.no_grad():        for data in testing_dataloader:            imgs, targets = data            outputs = net(imgs)            loss = loss_fn(outputs, targets)            total_testing_loss = total_testing_loss + loss.item()            accuracy = (outputs.argmax(1) == targets).sum()            total_accuracy = total_accuracy + accuracy    print(f&quot;Loss on the overall test set: &#123;total_testing_loss&#125;&quot;)    print(f&quot;Correctness on the overall test set: &#123;total_accuracy/testing_data_size&#125;&quot;)    writer.add_scalar(&quot;test_loss&quot;, total_testing_loss, total_testing_step)    writer.add_scalar(&quot;test_accuracy&quot;, total_accuracy/testing_data_size, total_testing_step)    total_testing_step += 1    torch.save(net, f&quot;net_&#123;i&#125;.pth&quot;)    # torch.save(net.state_dict(), f&quot;net_&#123;i&#125;.pth&quot;)    print(&quot;Models Saved&quot;)writer.close()\nHere is the complete file path\nFinally ü§©Thank you for the current age of knowledge sharing and the people willing to share it, thank you! The knowledge on this blog is what I‚Äôve learned on this site, thanks for the support! üòá\n"},{"title":"(CV) Read the ‚ÄúImageNet Classification with Deep Convolutional Neural Networks‚Äú paper.","url":"/2022/12/12/%5BCV%5D%20Read%20the%20%E2%80%9CImageNet%20Classification%20with%20Deep%20Convolutional%20Neural%20Networks%E2%80%9C%20paper.%20%EF%BC%88%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%8701%EF%BC%89/","content":"0. StatementThis paper is a very hot paper on computer vision published in 2012. I briefly sorted out the main points of the paper, if there are mistakes, welcome to correct them. Ôºà‚ÄùImageNet Classification with Deep Convolutional Neural Networks‚Äù. üè´\n1. Start reading the paper!According to my blog on how to read essays efficiently, we know that reading essays is divided into three steps. \n1.1 Pass 1Abstract -&gt; Discussion -&gt; graphs&#x2F; charts.After the first step, we know that the authors used CNNs to beat the other models in the competition, and the results were several times better than theirs. However, some details are also unclear to the authors and need to be discussed by the researchers in the future. \n1.2 Pass 2IntroductionWe want to do object recognition with very good models, prevent overfitting, and collect big data. It is easy to do big with CNNs, but easy to overfit. Because of the GPU, it is now good to train CNNs. Trained very large neural networks and achieved particularly good results. The neural network has 5 convolutional layers and 3 fully linked layers; it was found that depth is very important and removing any layer is not possible. And used unusual features and new techniques to reduce overfitting.\nThe DatasetThe Dataset has 15 million pieces of data and 20,000 categories. And this dataset does not have a fixed resolution, so the author made the image, 256 X 256. The short side is reduced to 256, and the long side is to ensure that the aspect ratio also goes down. However, if the long edge is more than 256, the two edges are truncated by the center. \nThe point ‚ö†Ô∏è: only the original pixels are used, and no data feature extraction is done. After that, this method is the so-called End to End which means that the original picture, the original text directly, does not do any feature extraction, the neural network can help to do it. \nThe ArchitectureCube indicates the size of the input and output of this data for each layer.  The data input is a 224 * 224 * 3 image. The first convolution layer (the window of convolution is 11 * 11 and the stride is 4) has 48 layers of channels. \nOn the engineering side, it is divided into two GPUs (GPU0, GPU1) with separate convolutional layers. Each GPU has 5 convolutional layers, and the output of one convolutional layer is the input of the other convolutional layer. In convolutional layers 2 and 3, there is communication between GPU0 and GPU1, which are merged in the output channel dimension. In the rest of 1 to 2, 3 to 4, and 4 to 5, each GUP does its learning. However, the height and width between the respective convolutional layers are varied.\nFeature ‚ö†Ô∏è: As the height and width of the image slowly decrease, the depth slowly increases. That is, the spatial information is slowly compressed as the network increases (224 * 224 to 13 * 13; indicating that each pixel inside 13 * 13 can represent a large chunk of the previous pixel). And the number of channels slowly increases (each channel number is a specific some patterns, 192 can be simply thought of as being able to identify the middle of picture 192 different patterns, each channel to identify a is a cat leg ah or an edge or something else what). After that, the input of the full linkage layer of each GPU is the result of combining the output of the 5th convolution of the two GPUs, each GPU does the full linkage independently first. Then, finally, it is combined at the classification layer to generate a vector of length 4096. If the vectors of 4096 are very close to each other, it means that the two images are of the same object. And the vector of 4096 lengths can represent the semantic information of the images well.\nIn summary\nThe increase in the width of the convolutional layers is an increase in semantic information. \nThe usefulness of deep learning is that, through so many previous layers, a picture is finally compressed into a single vector of 4096 lengths. This vector can represent all the semantic information in between. After a series of training (feature extraction) in the previous layers, it becomes a vector that the machine can understand. \nThe whole machine learning can be seen as the compression of knowledge (text, images, etc.), through the intermediate model, and finally compressed into a vector, this vector can be recognized by the machine. After the machine recognizes it, it can do all kinds of things on it.\n\nModel defectsAlexNet design with three full links (the last one is the output, the middle two very large 4096 full links is a big bottleneck, resulting in a particularly large model that can not put the network into the GPU as a whole).\nReducing OverfittingOverfitting image understanding: You are given some questions, and you memorize this, but there is no understanding of what the questions are for. So, the exam is not good.\nData Augmentation\nThe author set the image to 224 * 224, so the image is randomly snapped from 256 * 256. \nThe author changed the entire RGB channel of the image, using PCA, so that the transformed image will be different from the original (color).\n\nDetails of learning\nSGD: a batch size of 128 examples, momentum of 0.9 weight decay of 0.0005 (L2 regularization).\nThe weights are initialized by Gaussian random variables with mean of 0 and variance of 0.01.3.The bias of the convolution layers in the second, fourth, and fifth layers are initialized to 1, the fully linked layers are also initialized to 1, and the rest layers are initialized to 0. \nThe learning rate is initialized to 0.01; if the validation set error does not move, it is manually divided by 10 to 0.001.\n\nResults\nFound a strange phenomenon: the result after convolution on GPU0 is color independent, but on GPU1 is color-dependent. \nLook at each feature activation: that is, what the output of each convolutional layer or fully linked layer is doing. Very often, we do not know what the neurons are learning. However, some neurons correspond very well to the neurons at the bottom of the layer that has learned some more local information, such as texture, orientation, etc. If it‚Äôs a neuron on the upper side it will learn, for example, this is a hole, or this is a human‚Äòs head or hand, or this is an animal or a lot of information in there.\n\n2. Acknowledgements\nThanks to Mr. Li Mu for sharing, the video explanation is detailed in the following link üîó:Pass 1Pass 2\n\n"},{"title":"The confusion of the parameter‚Äúdim‚Äùin PyTorch.","url":"/2022/12/12/The%20confusion%20of%20the%20parameter%20%E2%80%9Cdim%E2%80%9D%20in%20Pytorch.%20_/","content":"Intro: When I was processing the data set and practicing how to change the data by dimension, the ‚Äúdim‚Äù confused me. üòØStart: Let‚Äôs read the codes. üö∂Congratulations, if you know why it‚Äôs happened. üéÜimport torcha = torch.ones(    (2, 5, 4))print(a.shape)print(&quot;value of scalar: &quot;,a.sum())print(&quot;scalar: &quot;,a.sum().shape)print(&quot;a: &quot;, a)print(&quot;axis=1&quot;, a.sum(axis=1))print(&quot;axis=1, keepdims=True: \\n&quot;, a.sum(axis=1, keepdims=True))print(&quot;axis=2&quot;, a.sum(axis=2))print(&quot;axis=2, keepdims=True: \\n&quot;, a.sum(axis=2, keepdims=True))print(&quot;axis=0&quot;, a.sum(axis=0))print(&quot;axis=0, keepdims=True: \\n&quot;, a.sum(axis=0, keepdims=True))print(&quot;axis=[0, 2]&quot;, a.sum(axis=[0, 2]))print(&quot;axis=[0, 2], keepdims=True: \\n&quot;, a.sum(axis=[0, 2], keepdims=True))\n\n\n1. Let‚Äôs focus on the ‚Äú.shape‚Äù.The ‚Äú.shape‚Äù has the ‚Äúindex‚Äù\n2. Let‚Äôs focus on Computing.when ‚Äúaxis&#x3D;1‚Äù your eyes should focus on the [ ] whose index is 1. and you can find the number of [[ ]] is 2. So, what are the fundamental elements of [ ] of index 2? The  [ ] of index 3! Perfect!! üëº\nConclusionThe others are the same things. The key thinking is that if you wanna compute the tensor which is changing by the ‚Äúdim‚Äù parameter. You should pay attention to the value of the ‚Äúdim‚Äù and through the ‚Äú.shape‚Äù index and the index of the tensor to get the true conclusion. üéâ"},{"title":"(NLP) Summary of the input details of embedding layer, LSTM layer.","url":"/2022/12/13/%5BNLP%5D%20Summary%20of%20the%20input%20details%20of%20embedding%20layer,%20LSTM%20layer./","content":"0. Statement. üòÑToday, I summarize the details of the problems I encountered through the NLP experiments I did before. Although it is a small detail, it will determine whether the experiment can go on or not, so it is very important for us. üè´\n1. Complete the classification problem of the iris dataset using LSTM + Embedding. üòáimport torchimport numpy as npimport pandas as pdfrom sklearn.metrics import accuracy_score, f1_scorefrom torch.utils.data.dataset import Datasetfrom torch.utils.data.dataloader import DataLoaderfrom torch import nn# Set random seedsseed = 1torch.manual_seed(seed)torch.cuda.manual_seed(seed)torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.np.random.seed(seed)  # Numpy module.torch.manual_seed(seed)# HyperparametersMax_epoch = 100batch_size = 6learning_rate = 0.05path = &quot;Iris.csv&quot;train_rate = 0.8df = pd.read_csv(path)# Use pandas to mess up the datadf = df.sample(frac=1)cols_feature = [&#x27;SepalLengthCm&#x27;, &#x27;SepalWidthCm&#x27;, &#x27;PetalLengthCm&#x27;, &#x27;PetalWidthCm&#x27;]df_features = df[cols_feature]df_categories = df[&#x27;Species&#x27;]# Convert category strings to numbers.categories_digitization = []for i in df_categories:\tif i == &#x27;Iris-setosa&#x27;:\t\tcategories_digitization.append(0)\telif i == &#x27;Iris-versicolor&#x27;:\t\tcategories_digitization.append(1)\telse:\t\tcategories_digitization.append(2)features = torch.from_numpy(np.float32(df_features.values))print(features)categories_digitization = np.array(categories_digitization)categories_digitization = torch.from_numpy(categories_digitization)print(&quot;categories_digitization&quot;,categories_digitization)# Define MyDataset class, inherit Dataset methods, and override __getitem__() and __len__() methodsclass MyDataset(Dataset):\t# Initialize the function and get the data\tdef __init__(self, datas, labels):\t\tself.datas = datas\t\tself.labels = labels\t# print(df_categories_digitization)\t# index is the index obtained after dividing the data according to the batchsize, and finally the data and the corresponding labels are returned together\tdef __getitem__(self, index):\t\tfeatures = self.datas[index]\t\tcategories_digitization = self.labels[index]\t\treturn features, categories_digitization\t# This function returns the length of the data size, the purpose of the DataLoader to facilitate the division, if you do not know the size, the DataLoader will be a confused face\tdef __len__(self):\t\treturn len(self.labels)num_train = int(features.shape[0] * train_rate)train_x = features[:num_train]train_y = categories_digitization[:num_train]test_x = features[num_train:]test_y = categories_digitization[num_train:]# Load the data through MyDataset and return the Dataset object, containing data and labelstrain_data = MyDataset(train_x, train_y)test_data = MyDataset(test_x, test_y)# Read datatrain_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0)test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0)class MyModel(nn.Module):\tdef __init__(self):\t\tsuper().__init__()\t\tself.embedding = nn.Embedding(num_embeddings=100, embedding_dim=300)\t\tself.lstm = nn.LSTM(input_size=300, hidden_size=128, num_layers=2, batch_first=True)\t\tself.fc = nn.Linear(128, 3)\tdef forward(self, x):\t\tprint(&quot;Just entered into the network:&quot;,x.shape)\t\tx = self.embedding(x)\t\tprint(&quot;After embedding&quot;,x.shape)\t\tx, _ = self.lstm(x)\t\tprint(&quot;After lstm&quot;, x.shape)\t\tx = self.fc(x[:, -1, :])\t\treturn x# Instantiated modelsmodel = MyModel()print(model)# Define loss functions and optimizerscriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters())# Training Modelfor epoch in range(10):\tfor step, batch in enumerate(test_dataloader):\t\tbatch_x, batch_y = batch\t\t# Obtain training data and perform pre-processing\t\t# inputs, targets = get_data()\t\t# inputs = torch.LongTensor(inputs)\t\tprint(&quot;batch_x.dtype:&quot;,batch_x.dtype)\t\tbatch_x = batch_x.type(torch.LongTensor)\t\tprint(&quot;batch_x.dtype:&quot;,batch_x.dtype)\t\tx = torch.LongTensor(batch_x)\t\t# targets = torch.FloatTensor(targets)\t\tprint(batch_y.dtype)\t\tbatch_y = batch_y.type(torch.LongTensor)\t\tprint(batch_y.dtype)\t\ty = torch.LongTensor(batch_y)\t\tprint(&quot;y.shape(type: classes):&quot;,y.shape)\t\t# Clear gradient\t\toptimizer.zero_grad()\t\t# Forward propagation\t\tprint(&quot;x.shape:&quot;,x.shape)\t\toutputs = model(x)\t\tprint(&quot;outputs.shape:&quot;, outputs.shape)\t\t# Calculated losses\t\tloss = criterion(outputs, y)\t\t# Back propagation\t\tloss.backward()\t\t# Update parameters\t\toptimizer.step()\t\t# Output the loss for each epoch\t\tprint(f&quot;epoch &#123;epoch+1&#125;: loss = &#123;loss.item()&#125;&quot;)model.eval()  #net.eval() is needed for testingtest_pred = []test_true = []for step, batch in enumerate(test_dataloader):\tbatch_x, batch_y = batch\tbatch_x = batch_x.type(torch.LongTensor)\tx = torch.LongTensor(batch_x)\tlogistics = model(x)\t# Take the largest value in each [].\tpred_y = logistics.argmax(1)\ttest_true.extend(batch_y.numpy().tolist())\ttest_pred.extend(pred_y.detach().numpy().tolist())print(f&quot;test set prediction &#123;test_pred&#125;&quot;)print(f&quot;test set exact&#123;test_true&#125;&quot;)accuracy = accuracy_score(test_true, test_pred)f1 = f1_score(test_true, test_pred, average = &quot;macro&quot;)print(f&quot;Accuracy:&#123;accuracy&#125;&quot;)print(f&quot;F1:&#123;f1&#125;&quot;)print(model)\n\n\n2. The shape of the embedding layer. ü§†The input shape of the embedding layer is (batch_size, sequence_length). In this case, batch_size represents the number of samples in the batch and sequence_length represents the length of the sequence in each sample. For example, for a text classification task, the sequence may contain words or sentences of words, and sequence_length is the number of words in the sentence.For example, in the following iris datasetüëá, I extract the first 6 samples as a batch, so batch_size &#x3D; 6, and there are 4 features in the dataset except for Id, so sequence_length is 4. In summary, (batch_size, sequence_length) &#x3D; (6, 4).\nId, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species1,5.1,3.5,1.4,0.2,Iris-setosa2,4.9,3.0,1.4,0.2,Iris-setosa3,4.7,3.2,1.3,0.2,Iris-setosa4,4.6,3.1,1.5,0.2,Iris-setosa5,5.0,3.6,1.4,0.2,Iris-setosa6,5.4,3.9,1.7,0.4,Iris-setosa\n3. The shape of the LSTM layer. ü•∏When batch_first&#x3D;True, the input shape of the LSTM layer is (batch_size, sequence_length, input_dim). In this case, batch_size denotes the number of samples in the batch, sequence_length denotes the length of the sequence in each sample, and input_dim denotes the number of features in each cell. For example, for a text classification task, the sequence may contain words or sentences composed of words. input_dim is the number of features in each word (e.g., the dimensionality of the word vector).The size of num_embeddings is related to the size of the vocabulary. num_embeddings refers to the number of different words in the vocabulary and is used to initialize the word embedding matrix, so if the vocabulary is large, then num_embeddings should be increased accordingly.üëá\n4. Why use x &#x3D; self.fc(x[:, -1, :])? ü§ìThe reason for using x[:, -1, :] is that the LSTM layer processes each cell in the sequence through a circular mechanism and remembers the previous information in the sequence, so the last cell in the sequence is usually considered to contain the information in the whole sequence and is more easily classified by the classifier.\nFinally ü§©Thank you for the current age of knowledge sharing and the people willing to share it, thank you!\n"},{"title":"(NN) What is Gradient exploding & Gradient vanishing? and how to solve it?","url":"/2022/12/12/%5BNN%5D%20What%20is%20Gradient%20exploding%20&%20Gradient%20vanishing_%20and%20how%20to%20solve%20it_/","content":"Today I intend to discuss gradient explosion and vanishing issues. üßê1. An intuitive understanding of what gradient explosion and gradient disappearance are. ü§îYou and I know about when the person who does more things than yesterday and develops himself can get crazy successful. I want to organize this thing to map with math.what is the 0.99 to 100th power and 1.1 to 100th power?(0.99 ** 100 &#x3D; 0.3660323413) And (1.1 ** 100 &#x3D; 13,780.6123398223) What a big difference! ü§† If you know anything about calculus, you know that if you multiply successively by a number greater than 1 you will get close to infinity (Gradient exploding), and conversely multiplying by a number less than 1 you will get close to 0 (Gradient vanishing).\n2. Why the neural network should multiply successively by a number? (Causes gradient vanishing and gradient explosion.ü§®)When we train a neural network, it involves chain rule and gradient descent (All of these require multiplication).üò≠ Usually, the neural network we are about to train will almost always contain nonlinear activation functions (sigmoid, tanh, etc.).This ‚Äòsigmoid‚Äô graph is from here.This ‚Äòtanh‚Äô graph is from here.This ‚ÄòReLu‚Äô graph is from here.Based on the three graphs above, I can conclude that there is a risk of gradient vanishing if the sigmoid function and the tanh function are used. Because their values are less than 1, but the ReLu function is an exception.ü•≤\n3. How do I solve the gradient explosion? ü§ï(Gradient clipping)I will choose to use gradient clipping. first, I will set a gradient threshold, suppose it is set to 15. after that, I need to check the gradient of weight, if this gradient is greater than 15, I will process it according to the formula in the picture above. In detail, I will divide the tensor of the current gradient by the norm of this tensor. According to this formula, we will get a tensor of size 1 and the same direction as before, and then multiply it by 15. The result is that we can constrain it to 15. Because the direction represents the direction of gradient descent and the norm represents the length of the step. ü§ó\nHow do I use gradient clipping in PyTorch? ü§†loss = criterion(output, y)model.zero_grad()loss.backward()for p in model.parameters():    print(p.grad.norm())    torch.nn.utils.clip_grad_norm(p, max_norm=10)optimizer.step()\n4. List some solutions that can mitigate gradient explosion and gradient vanishing. üßê\nGradient clipping.\nWeight decay.\n\noptimizer = optim.Adam(model.parameters(),lr=0.05,weight_decay=0.01)\n\n\nBatchnorm: The distribution of the input values of the neurons in the backpropagation is changed to a standard normal distribution with a mean of 0 and a variance of 1. This treatment leads to a large change in the loss function, which makes the gradient larger and avoids the gradient disappearance problem.\nUse activation functions such as ReLu, ELu, etc.\nShortcut\nSome neural networks with ‚Äúgates‚Äù. (LSTM, GRU, etc.)\nAppropriate control of the number of layers of the neural network.\nAnd so on‚Ä¶\n\nComplete Codeimport numpy as npimport torchfrom torch import nnimport torch.optim as optimfrom matplotlib import pyplot as pltimport osos.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;num_time_steps = 50input_size = 1hidden_size = 16output_size = 1lr = 0.01class Net(nn.Module):    def __init__(self)-&gt; None:        super().__init__()        self.rnn = nn.RNN(            input_size=input_size,            hidden_size=hidden_size,            num_layers=1,            batch_first=True,        )        for p in self. rnn.parameters():            nn.init.normal_(p, mean=0.0, std=0.001)        self.linear = nn.Linear(hidden_size, output_size)    def forward(self, x, hidden_prev):        out, hidden_prev = self.rnn(x, hidden_prev)        # [1, seq, h] =&gt; [seq, h]        out = out.view(-1, hidden_size)        out = self.linear(out) # [seq, h] =&gt; [seq, 1]        out = out.unsqueeze(dim=0) # =&gt; [1, seq, 1]        return out, hidden_prevmodel = Net()criterion = nn.MSELoss()optimizer = optim.Adam(model.parameters(), lr)hidden_prev = torch.zeros(1, 1, hidden_size)for iter in range(6000):    start = np.random.randint(3, size=1)[0]    time_steps = np.linspace(start, start+10, num_time_steps)    data = np.sin(time_steps)    data = data.reshape(num_time_steps, 1)    x = torch.tensor(data[:-1]).float().view(1, num_time_steps -1, 1)    y = torch.tensor(data[1:]).float().view(1, num_time_steps -1, 1)    output, hidden_prev = model(x, hidden_prev)    hidden_prev = hidden_prev.detach()    loss = criterion(output, y)    model.zero_grad()    loss.backward()    for p in model.parameters():        print(p.grad.norm())        torch.nn.utils.clip_grad_norm(p, max_norm=10)    optimizer.step()    if iter % 100 ==0:        print(f&quot;Iteration:&#123;iter&#125;  loss&#123;loss.item()&#125;&quot;)start = np.random.randint(3, size=1)[0]time_steps = np.linspace(start, start + 10, num_time_steps)data = np.sin(time_steps)data = data.reshape(num_time_steps, 1)x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)predictions = []input = x[:, 0, :]for _ in range(x.shape[1]):    input = input.view(1, 1, 1)    (pred, hidden_prev) = model(input, hidden_prev)    input = pred    predictions.append(pred.detach().numpy().ravel()[0])x = x.data.numpy().ravel()y = y.data.numpy()plt.scatter(time_steps[:-1], x.ravel(), s=90)plt.plot(time_steps[:-1], x.ravel())plt.scatter(time_steps[1:], predictions)plt.show()\n\nFinally ü§©Thank you for the current age of knowledge sharing and the people willing to share it, thank you! The knowledge on this blog is what I‚Äôve learned on this site, thanks for the support!  üòá\n"},{"title":"(NLP) Description and implementation of LSTM neural network.","url":"/2022/12/13/%5BNLP%5D%20Description%20and%20implementation%20of%20LSTM%20neural%20network./","content":"0. Statement üè´Today I intend to move from an intuitive understanding of LSTM to its implementation with PyTorch, and I believe readers can get substantial help through this blog.\n1. What is the advantage of LSTM over RNN? ü§îRNN can remember fewer words related to the context than LSTM, so RNN is often called a short-term neural network, while LSTM is a Long short-term neural network, where long means that it can remember more context than RNN which represents the short-term neural network. So you don‚Äôt have to be confused by the name(Long Short-term). ü§ó\n2. Differences in terminology between RNN and LSTM for picture representation. üßêThe RNN and LSTM pictures are from here.\n\nThe output in RNN is ot, while the output of LSTM is ht.\nThe contextual information(Memory) in RNN is stored in ht(above), while the contextual information(Memory) in LSTM is stored in ct.\n\n3. The composition and intuitive understanding of LSTM. ü§† When I first saw the architecture diagram of LSTM, I noticed a schematic of sigmoid and multiplication together.The sigmoid takes values from 0 to 1, which means that certain numbers are multiplied by 0 or 1, which means that the significance of each of these structures is to decide whether to use the data from the previous time step.This structure is named ‚Äúgate‚Äù.\n3.1. LSTM: Forget gate.When the value of ‚Äúft‚Äù is 1, it means I want to use the data remembered in the previous time step, and when the value is 0, it means I want to forget it.\n3.2. LSTM: Input gate and Cell State.When the value of ‚Äúit‚Äù is 1, it means that I want to use the data entered at the current time (‚ÄúC wave t‚Äù), which is calculated by ‚Äútanh‚Äù, ‚ÄúWc‚Äù and ‚Äúbc‚Äù based on the data entered at the current time ‚Äúxt‚Äù.‚ö†Ô∏èÔºö In summary, the forgetting gate determines whether the information remembered at the previous time step is useful, and the inputting gate determines whether the information to be remembered at the current time step is important.\n3.3. LSTM: Output.In summary, the output ‚Äúht‚Äù of the LSTM is the element-wise product of the ‚Äútanh‚Äù operation of ‚Äúct‚Äù and the ‚Äúoutput gate‚Äù.\n3.4. SummaryThe forgetting gate, input gate, and output gate require the sigmoid and the input x(t) at the current time and the context information h(t-1) from the previous time step. To input the c-wave into the cell requires tanh and x(t), h(t-1). The c(t) and h(t) to be passed to the next time step are easy to understand intuitively based on the above diagram. üòá\n\n3.5. Understanding the role of ‚Äúgates‚Äù intuitively.3.6. Why can LSTM mitigate gradient vanishing?Because when we solve for the gradient, we avoid the appearance of the kth power of ‚ÄúWhh‚Äù, and then because there are three ‚Äúgates‚Äù, we need to expand three equations when solving for the gradient, and the three gates constrain each other so that the probability of a large or small value is much smaller.\n4. How to implement LSTM with PyTorch? üòéThe PyTorch documentation on LSTM is from here.\nimport torchfrom torch import nnlstm = nn.LSTM(input_size=100, hidden_size=20, num_layers=4)print(lstm)x = torch.randn(10, 3, 100)out, (h, c) = lstm(x)print(out[-1])print(h[-1])print(out[-1].shape)print(h[-1].shape)print(out[-1]==h[-1])print(f&quot;out.shape:&#123;out.shape&#125;\\nh.shape:&#123;h.shape&#125;\\nc.shape:&#123;c.shape&#125;&quot;)\n‚ö†Ô∏è: The out of the LSTM is the value of the last time step h of all time steps h. ü§†\nimport torchfrom torch import nnlstm = nn.LSTM(input_size=100, hidden_size=20, num_layers=4)print(lstm)x = torch.randn(10, 3, 100)out, (h, c) = lstm(x)print(out[-1])print(h[-1])print(out[-1].shape)print(h[-1].shape)print(out[-1]==h[-1])print(f&quot;out.shape:&#123;out.shape&#125;\\nh.shape:&#123;h.shape&#125;\\nc.shape:&#123;c.shape&#125;&quot;)print(&#x27;one layer lstm&#x27;)cell = nn.LSTMCell(input_size=100, hidden_size=20)h = torch.zeros(3, 20)c = torch.zeros(3, 20)for xt in x:    h, c = cell(xt, [h, c])print(f&quot;h.shape:&#123;h.shape&#125;&quot;)print(f&quot;c.shape:&#123;c.shape&#125;&quot;)print(&quot;two layer lstm&quot;)cell1 = nn.LSTMCell(input_size=100, hidden_size=30)cell2 = nn.LSTMCell(input_size=30, hidden_size=20)h1 = torch.zeros(3, 30)c1 = torch.zeros(3, 30)h2 = torch.zeros(3, 20)c2 = torch.zeros(3, 20)for xt in x:    h1, c1 = cell1(xt, [h1, c1])    h2, c2 = cell2(h1, [h2, c2])print(f&quot;h2.shape:&#123;h2.shape&#125;&quot;)print(f&quot;c2.shape:&#123;c2.shape&#125;&quot;)\n\nFinally ü§©Thank you for the current age of knowledge sharing and the people willing to share it, thank you! The knowledge on this blog is what I‚Äôve learned on this site, thanks for the support! üòá\n"},{"title":"(Reinforcement Learning) The Study note of Reinforcement Learning.","url":"/2023/01/13/%5BReinforcement%20Learning%5D%20The%20Study%20note%20of%20Reinforcement%20Learning./","content":"0. Statement ü´°This blog is my self-study study notes on reinforcement learning based on this website. After that, I organized some of the more core and important contents of the course. üßê\nStart ü§†Reinforcement Learning. - Reinforcement learning algorithms seek to find a policy that will yield more return to the agent than all other policies.\n\nA Markov decision - Agent, Environment, State, Action, Reward.The goal of an agent in an MDP is to maximize its cumulative rewards.\nThe cumulative rewards.The expected return is what‚Äôs driving the agent to make the decisions it makes.\n\n\nIt is the agent‚Äôs goal to maximize the expected return of rewards.\n\n\nTasks\nEpisodic tasks - The agent-environment. interaction naturally breaks up into subsequences. Such as the ping-pong game. \nContinuing tasks.\nDiscounted Return - Rather than the agent‚Äôs goal being to maximize the expected return of rewards, it will instead be to maximize the expected discounted return of rewards. The agent will be choosing action A-t at each time t to maximize the expected discounted return.\n\n\n\n\n\n\nIt is the agent‚Äôs goal to maximize the expected discounted return of rewards.\nThe discount rate will be the rate for which we discount future rewards and will determine the present value of future rewards. This definition of the discounted return makes it to where our agent will care more about the immediate reward over future rewards since future rewards will be more heavily discounted.\nThe infinite sum of discounted returns is finite if the conditions as long as the reward are nonzero and constant,  and lambda less than 1.\n\n\nPolicy(œÄ) - How probable is it for an agent to select any action from a given state?\nDefinition: A policy is a function that maps a given state to probabilities of selecting each possible action from that state.\n\n\nValue functions - How good is any given or any given state for an agent?\nDefinition: Value functions are functions of states, or of state-action pairs, that estimate how good it is for an agent to be in a given state, or how good it is for the agent to perform a  given action in a given state.\n\n\nState-Value  Function(VœÄ) VœÄ(s)\nThe state-value function for policy œÄ, denoted as VœÄ, tells us how good any given state is for an agent following policy œÄ. (It gives us the value of a state under œÄ)\n\n\nAction-Value Function(qœÄ) qœÄ(s, a)\nThe action-value function for policy œÄ,  denoted as qœÄ, tells us how good it is for the agent to take any given action from a given state while following policy œÄ. (It gives us the value of action under œÄ)\nThe action-value function qœÄ is referred to as the Q-function, and the output from the function for any given state-action pair is called a Q-value. (The letter ‚ÄúQ‚Äù is used to represent the quality of taking a given action in a given state.)\n\n\nWrapping up\nThe state-value function tells us how good any given state is for the agent, whereas the action-value function tells us how good it is for the agent to take any action from a given state.\nValue functions are defined with respect to the expected return, specific ways of acting, and the policy.\n\n\nOptimal policies. - A policy that is better than or at least the same as all other policies.\nOptimal State-value Function V*: V* gives the largest expected return achievable by any policy œÄ for each state.\nOptimal Acton-Value Function q*: q* gives the largest expected return achievable by any policy œÄ for each possible state-action pair.\nQ-learning: It is a reinforcement learning technique used for learning the optimal policy in a Markov Decision Process.\nThe goal of Q-learning is to find the optimal policy by learning the optimal Q-values for each state-action pair.\nExploration Vs. Exploitation: \nExploration is the act of exploring the environment to find out information about it.\nExploitation is the act of exploiting the information that is already known about the environment in order to maximize the return.\nTo get this balance between exploitation and exploration, we use what is called an epsilon greedy strategy.\n\n\n\n\nChoosing Actions with An Epsilon Greedy Strategy.\nWith this strategy, we define an exploration rate epsilon that we initially set to 1. This exploration rate is the probability that our agent will explore the environment rather than exploit it. With epsilon &#x3D; 1, it is 100% certain that the agent will start out by exploring the environment.As the agent learns more about the environment, at the start of each new episode, epsilon will decay by some rate that we set so that the likelihood of exploration becomes less and less probable as the agent learns more and more about the environment. \nWe generate a random number between 0 and 1. If this number is greater than epsilon, then the agent will choose its next action via exploitation, i.e. it will choose the action with the highest Q-value for its current state from the Q-table. Otherwise, its action will be chosen via exploration, i.e. randomly choosing its action and exploring what happens in the environments.\n\n\nAll of the steps for Q-learning.\nInitialize all Q-values in the Q-table to 0.\nFor each time-step in each episode:\nChoose an action (Considering the exploration-exploitation trade-off).\nObserve the reward and next state.\nUpdate the Q-value function(Make the Q-value function converge to the right hand side of the Bellman equation).\n\n\n\n\n\nimport numpy as npimport gymimport randomimport timefrom IPython.display import clear_outputenv = gym.make(&#x27;FrozenLake-v1&#x27;, render_mode=&#x27;ansi&#x27;)action_space_size = env.action_space.nstate_space_size = env.observation_space.nq_table = np.zeros(    (state_space_size, action_space_size))a = np.zeros(    (1, 2))print(q_table)num_episodes = 1000max_steps_per_episode = 100learning_rate = 0.1discount_rate = 0.99exploration_rate = 1max_exploration_rate = 1min_exploration_rate = 0.01exploration_decay_rate = 0.001rewards_all_episodes = []# Q-learning algorithmfor episode in range(num_episodes):    state = env.reset()[0]    done = False    rewards_current_episode = 0    for step in range(max_steps_per_episode):        # Exploration-exploitation trade-off        exploration_rate_threshold = random.uniform(0, 1)        if exploration_rate_threshold &gt; exploration_rate:            action = np.argmax(q_table[state,:])        else:            action = env.action_space.sample()        new_state, reward, done, truncated, info = env.step(action)        # Update Q-table for Q(s, a)        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))        state = new_state        rewards_current_episode += reward        if done == True:            break    # Exploration rate decay    exploration_rate = min_exploration_rate + \\                       (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)    rewards_all_episodes.append(rewards_current_episode)# Calculate and print the average reward per thousand episodesrewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 1000)count = 1000print(&quot;======== Average reward per thousand episodes========\\n&quot;)for r in rewards_per_thousand_episodes:    print(count, &quot;: &quot;, str(sum(r / 1000)))    count += 1000# print updated Q-tableprint(&quot;\\n\\n========Q-table========\\n&quot;)print(q_table)\n\n\n\n"},{"title":"(NLP) Predicting trigonometric functions with RNNs.","url":"/2022/12/12/%5BNLP%5D%20Predicting%20trigonometric%20functions%20with%20RNNs/","content":"Today I want to organize what I have learned about RNNs in the form of my blog.üëº1. Key pointRNNs are neural networks being affected by h(t-1) and x(t).What is h(t-1) and x(t)? This is my vivid answer. When we were in childhood, our parents used to request us to recite the textbooks.üò≠  There is a sentence below.üòö\n‚ÄúLet perseverance be your engine and hope your fuel!‚Äù‚ÄúÏùòÏßÄÎ†•Ïù¥ ÎãπÏã†Ïùò ÎèôÎ†•Ïù¥Ïó¨ Ìù¨ÎßùÏùÄ Ïó∞Î£åÎ°ú ÎêòÏñ¥Îùº!‚Äù‚ÄúËÆ©ÊØÖÂäõÊàê‰∏∫‰Ω†ÁöÑÂºïÊìéÔºåËÆ©Â∏åÊúõÊàê‰∏∫‰Ω†ÁöÑÁáÉÊñôÔºÅ‚ÄùLet‚Äôs start! how can you recite this sentence? üëÜ\n\nYou will only recite one word at a time, with only one individual word in your head. When your parents test you you can only memorize individual words that are divided from each other.Let‚Ä¶. ??? be ??? your ???? ????? fuel. üò† (PS: This is a purely linear layer if we think in terms of neural networks.Using linear layers for long sequences has the following drawbacks. The first is that there are too many parameters[w, b]. The second point is that contextual information cannot be preserved.)\nYou can recall a word you have memorized before you start to recite the word of the moment, and continue in this way. When you recite the word ‚Äúperseverance‚Äù you need to recall the word ‚Äúlet‚Äù first. In other words, you need to remember what happened a moment before the current time.\n\nTo sum up, ‚Äú perseverance ‚Äú means x(t). When the moment t is the word ‚Äúperseverance‚Äù you want to recite, the ‚ÄúLet‚Äù in your mind the moment before is h(t-1). The point is that the word must be in your mind and be absorbed into your cells, if not, it is simply x(t-1). üç∞From here\n2. Using RNNs to predict the sin function.\nimport numpy as npimport torchfrom torch import nnimport torch.optim as optimfrom matplotlib import pyplot as pltimport osos.environ[&quot;KMP_DUPLICATE_LIB_OK&quot;]=&quot;TRUE&quot;num_time_steps = 50input_size = 1hidden_size = 16output_size = 1lr = 0.01class Net(nn.Module):    def __init__(self):        super().__init__()        self.rnn = nn.RNN(            input_size=input_size,            hidden_size=hidden_size,            num_layers=1,            batch_first=True,        )        for p in self.rnn.parameters():            nn.init.normal_(p, mean=0.0, std=0.001)        self.linear = nn.Linear(hidden_size, output_size)    def forward(self, x, hidden_prev):        out, hidden_prev = self.rnn(x, hidden_prev)        # [1, seq, h] =&gt; [seq, h]        out = out.view(-1, hidden_size)        out = self.linear(out) # [seq, h] =&gt; [seq, 1]        out = out.unsqueeze(dim=0) # =&gt; [1, seq, 1]        return out, hidden_prevmodel = Net()criterion = nn.MSELoss()optimizer = optim.Adam(model.parameters(), lr)hidden_prev = torch.zeros(1, 1, hidden_size)for iter in range(6000):    start = np.random.randint(3, size=1)[0]    time_steps = np.linspace(start, start+10, num_time_steps)    data = np.sin(time_steps)    data = data.reshape(num_time_steps, 1)    x = torch.tensor(data[:-1]).float().view(1, num_time_steps -1, 1)    y = torch.tensor(data[1:]).float().view(1, num_time_steps -1, 1)    output, hidden_prev = model(x, hidden_prev)    hidden_prev = hidden_prev.detach()    loss = criterion(output, y)    model.zero_grad()    loss.backward()    optimizer.step()    if iter % 100 ==0:        print(f&quot;Iteration:&#123;iter&#125;  loss&#123;loss.item()&#125;&quot;)start = np.random.randint(3, size=1)[0]time_steps = np.linspace(start, start + 10, num_time_steps)data = np.sin(time_steps)data = data.reshape(num_time_steps, 1)x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)predictions = []input = x[:, 0, :]for _ in range(x.shape[1]):    input = input.view(1, 1, 1)    (pred, hidden_prev) = model(input, hidden_prev)    input = pred    predictions.append(pred.detach().numpy().ravel()[0])x = x.data.numpy().ravel()y = y.data.numpy()plt.scatter(time_steps[:-1], x.ravel(), s=90)plt.plot(time_steps[:-1], x.ravel())plt.scatter(time_steps[1:], predictions)plt.show()\n\n\nIf you get this ERROR below.\nYou should be using the following method.import osos.environ[&#x27;KMP_DUPLICATE_LIB_OK&#x27;]=&#x27;True&#x27;\nFinally ü§©Thank you for the current age of knowledge sharing and the people willing to share it, thank you! The knowledge on this blog is what I‚Äôve learned on this site, thanks for the support! üòá\n"},{"title":"(NLP) Teach RNN to output ‚Äòihello‚Äò when ‚Äòhihell‚Äò is entered.","url":"/2022/12/12/%5BNLP%5D%20Teach%20RNN%20to%20output%20%E2%80%98ihello%E2%80%98%20when%20%E2%80%98hihell%E2%80%98%20is%20entered./","content":"0. Statement üëºThis blog is a summary of what I have learned, about using RNNs, embedding, PyTorch, etc., to complete RNNs experiments. If there are any mistakes, welcome to correct them. üè´\n1. Start üç∞\nThe following figure shows a general overview of the entire neural network, with some details not shown specifically. üëá\nThe following figure shows the problem to be solved in this experiment: when a sequence is input, the other correct Sequence is the output. üëá\n\n2. Dataset üê∂\nBuilding an overall frequency dictionary.\nBuilding a list of index-to-character mappings. (idx2char)\nBuilding a list of character-to-index mappings. (char2idx)# Original Textinput_text = &quot;hihell&quot;output_text = &quot;ihello&quot;# vocabularychar_freq = Counter(input_text + output_text)print(&quot;char_freq:&quot;,char_freq) # Counter(&#123;&#x27;l&#x27;: 4, &#x27;h&#x27;: 3, &#x27;i&#x27;: 2, &#x27;e&#x27;: 2, &#x27;o&#x27;: 1&#125;)print(&quot;char_freq.keys():&quot;, char_freq.keys()) # dict_keys([&#x27;h&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;o&#x27;])# idx2char = [c for c in char_freq.keys()]idx2char = [c for i, c in enumerate(char_freq.keys())]print(&quot;idx2char:&quot;,idx2char) # [&#x27;h&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;o&#x27;]char2idx = &#123;c: i for i, c in enumerate(idx2char)&#125;print(&quot;char2idx:&quot;,char2idx)# Vectors of input and outputinput_idx = [char2idx[c] for c in input_text]print(&quot;input_idx:hihell&quot;, input_idx) # input_idx: [0, 1, 0, 2, 3, 3]output_idx = [char2idx[c] for c in output_text]print(&quot;output_idx:ihello&quot;,output_idx) # output_idx [1, 0, 2, 3, 3, 4]\n\n3. Overall Architecture ü•∏Visualization of RNN. üëáFrom here\nimport torchfrom torch import nnimport numpy as npclass Net(nn.Module):    def __init__(self):        super().__init__()        self.model0 = nn.Sequential(            nn.Embedding(num_embeddings=6, embedding_dim=8),            nn.RNN(input_size=8, hidden_size=5)        )        self.model1 = nn.Sequential(            nn.Flatten(),            nn.Linear(in_features=5, out_features=5)        )    def forward(self, x):        x = self.model0(x)        x = self.model1(x[0])        return x\n\n4. Details ü§ì4.1 Input of shape üßê\nForward function of RNN\n‚ö†Ô∏è: Input of shape of RNN (seq_len, batch, input_size) - Because the data to be input is ‚Äòhihell‚Äô, so the seq_len is 6 because it is a sequence so the batch is 1, after the feature_len needs to be determined by the embedding layer.inputs = torch.tensor(input_idx).reshape(6, 1)print(inputs.shape) # torch.Size([6, 1])labels = torch.tensor(output_idx) # torch.Size([6])print(labels.shape)\n\nimport torch.nn as nnimport torch# There are 6 word vectors, each word vector is 8-dimensionalembeds = nn.Embedding(num_embeddings=6, embedding_dim=8)# Give the index of each word of a sentence (index)a = torch.tensor([0, 1, 0, 2, 3, 3]).reshape(6, 1)print(embeds(a).size()) # torch.Size([6, 1, 8])\n4.2 Flatten Layer üòáIf do not have Flatten Layer, you will get the shape of output(6 * 1 * 5).\nElse,  you will get the shape of output(6 * 5).\n‚ö†Ô∏èÔºö What is the difference between (6 * 1 * 5) and (6 * 5)?When you put the (6 * 1 * 5) into the Loss Function, you will get an Error. ASo Let‚Äôs check the Loss Function in PyTorch documents. From hereIf your targets are just 1-dim, your input should be 2-dim.\n5. Complete code and conclusion ü•≥MyNet.py\nimport torchfrom torch import nnimport numpy as npclass Net(nn.Module):    def __init__(self):        super().__init__()        self.model0 = nn.Sequential(            nn.Embedding(num_embeddings=6, embedding_dim=8),            nn.RNN(input_size=8, hidden_size=5)        )        self.model1 = nn.Sequential(            nn.Flatten(),            nn.Linear(in_features=5, out_features=5)        )    def forward(self, x):        x = self.model0(x)        x = self.model1(x[0])        return x\n\nRNNs_Learning.py\nfrom collections import Counterimport torchfrom torch import nnimport torch.nn.functional as Ffrom MyNet import *# Original textinput_text = &quot;hihell&quot;output_text = &quot;ihello&quot;# vocabularychar_freq = Counter(input_text + output_text)print(&quot;char_freq:&quot;,char_freq) # Counter(&#123;&#x27;l&#x27;: 4, &#x27;h&#x27;: 3, &#x27;i&#x27;: 2, &#x27;e&#x27;: 2, &#x27;o&#x27;: 1&#125;)print(&quot;char_freq.keys():&quot;, char_freq.keys()) # dict_keys([&#x27;h&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;o&#x27;])# idx2char = [c for c in char_freq.keys()]idx2char = [c for i, c in enumerate(char_freq.keys())]print(&quot;idx2char:&quot;,idx2char) # [&#x27;h&#x27;, &#x27;i&#x27;, &#x27;e&#x27;, &#x27;l&#x27;, &#x27;o&#x27;]char2idx = &#123;c: i for i, c in enumerate(idx2char)&#125;print(&quot;char2idx:&quot;,char2idx)# Vectors of input and outputinput_idx = [char2idx[c] for c in input_text]print(&quot;input_idx:hihell&quot;, input_idx) # input_idx: [0, 1, 0, 2, 3, 3]output_idx = [char2idx[c] for c in output_text]print(&quot;output_idx:ihello&quot;,output_idx) # output_idx [1, 0, 2, 3, 3, 4]# traininginputs = torch.tensor(input_idx).reshape(6, 1)print(inputs.shape) # torch.Size([6, 1])labels = torch.tensor(output_idx) # torch.Size([6])print(labels.shape)# creating networknet = Net()print(net)# output = net(inputs)# print(output)# print(output.shape)# Creating loss functionsloss_fn = nn.CrossEntropyLoss()# Opimizeroptimizer = torch.optim.Adam(net.parameters(), lr=0.01)pre_idx = []for i in range(1000):    print(f&quot;------- the &#123;i+1&#125;th round of training starts -------&quot;)    # Training steps begin    net.train()    print(inputs)    outputs = net(inputs)    # print(outputs.shape)    # print(output_idx)    loss = loss_fn(outputs, labels)    # Optimizer optimization model    optimizer.zero_grad()    loss.backward()    optimizer.step()    print(loss)    print(outputs)    print(outputs.argmax(1))    pre_idx.extend(outputs.argmax(1))for i in pre_idx[-6:]:    print(idx2char[i])\n\n"},{"title":"Using SVM+Word2Vec to solve the problem of classifying good and bad reviews of takeaways","url":"/2022/12/12/%E4%BD%BF%E7%94%A8SVM+Word2Vec%20%E8%A7%A3%E5%86%B3%E5%A4%96%E5%8D%96%E7%9A%84%E5%A5%BD%E8%AF%84%E5%9D%8F%E8%AF%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/","content":"0. StatementThis paper is a summary of what I have learned, using Sklearn, gensim, jieba, etc., to complete the classification experiments. If there are any mistakes, welcome to correct them. üè´\n1. Overall Architectureimport joblibimport jiebaimport gensimfrom numpy import *import numpy as npimport pandas as pdfrom sklearn import svmfrom sklearn.metrics import accuracy_score, f1_scorefrom sklearn.metrics import confusion_matrix# Loading deactivation wordsdef stop_words(path=&#x27;/Users/apple/PycharmProjects/Êú∫Âô®Â≠¶‰π†È°πÁõÆÂÆûËÆ≠/Â§ñÂçñ/stopwords.txt&#x27;):    with open(path,&#x27;r&#x27;,encoding=&#x27;gbk&#x27;,errors=&#x27;ignore&#x27;) as f:        return[l.strip() for l in f]# Text pre-processingdef text_preprocessing():    # Loading deactivation words    stopwords = stop_words()    # Read files    df = pd.read_csv(&quot;/Users/apple/PycharmProjects/Êú∫Âô®Â≠¶‰π†È°πÁõÆÂÆûËÆ≠/Â§ñÂçñ/waimai_10k.csv&quot;)    # Cut words and filter mediation words    df[&quot;review&quot;] = df[&quot;review&quot;].map(lambda x: &quot; &quot;.join([i for i in jieba.cut(x) if i not in stopwords]))    # Save the processed text    df.to_csv(&quot;/Users/apple/PycharmProjects/Êú∫Âô®Â≠¶‰π†È°πÁõÆÂÆûËÆ≠/Â§ñÂçñ/waimai.csv&quot;, index=False, header=False,              columns=[&quot;label&quot;, &quot;review&quot;])# Data set divisiondef partition_data_set():    data = pd.read_csv(&quot;/Users/apple/PycharmProjects/Êú∫Âô®Â≠¶‰π†È°πÁõÆÂÆûËÆ≠/Â§ñÂçñ/waimai.csv&quot;, header=None)  #Âä†ËΩΩÊï∞ÊçÆ    data:pd.DataFrame = data.sample(frac=1.0)    ##Break up the data    rows, cols = data.shape    # train: val:test = 7:1: 2    split_index_1 = int(rows * 0.2)    split_index_2 = int(rows * 0.3)    #data segmentation    data_test:pd.DataFrame = data.iloc[0: split_index_1, :]    data_validate:pd.DataFrame = data.iloc[split_index_1:split_index_2, :]    data_train:pd.DataFrame = data.iloc[split_index_2: rows, :]    #data saving    data_test.to_csv(&quot;test.csv&quot;, header=None, index=False)    data_validate.to_csv(&quot;validate.csv&quot;, header=None, index=False)    data_train.to_csv(&quot;train.csv&quot;, header=None, index=False)    print(&quot;The delineation is complete&quot;)# Get the tags of the dataset and the corresponding data (darry format)def get_label_And_review(file):    data = pd.read_csv(file, header=None)    return data[0].values, data[1].values&#x27;&#x27;&#x27;=============================================================&#x27;&#x27;&#x27;# Get word vectorsdef get_wordvec(data_review, filedesignation):    model = gensim.models.Word2Vec(data_review, vector_size=128, workers=4, min_count=0)    model.wv.save_word2vec_format(f&quot;word_vec_&#123;filedesignation&#125;.txt&quot;, binary=False)    return model# We know that the superposition of word vectors also superposes the semantics, and here we add all the words in each sentence to the word vector, and we can define the method of adding word vectors.def total_vector(words, model):    vec = np.zeros(128).reshape((1, 128))    for word in words:        try:            vec += model.wv[word].reshape((1, 128))        except KeyError:            continue    return vecif __name__ == &#x27;__main__&#x27;:    text_preprocessing()    partition_data_set()    all_label, all_review = get_label_And_review(&quot;waimai.csv&quot;)    train_label, train_review = get_label_And_review(&quot;train.csv&quot;)    test_label, test_review = get_label_And_review(&quot;test.csv&quot;)    validate_label, validate_review = get_label_And_review(&quot;validate.csv&quot;)    model_all = get_wordvec(all_review, &quot;all_review&quot;)    model_train = get_wordvec(train_review,&quot;train_review&quot;)    train_vec = np.concatenate([total_vector(words, model_train) for words in train_review])    model_test = get_wordvec(test_review,&quot;test_review&quot;)    test_vec = np.concatenate([total_vector(words, model_train) for words in test_review])    model_validate = get_wordvec(validate_review, &quot;validate_review&quot;)    validate_vec = np.concatenate([total_vector(words, model_train) for words in validate_review])    svm = svm.SVC()    svm.fit(train_vec, train_label)    pre_test_label = svm.predict(test_vec)  # prediction    ac_test = accuracy_score(pre_test_label, test_label)    print(f&quot;accuracy:&#123;ac_test&#125;&quot;)    svm.score(test_vec, test_label)    joblib.dump(svm, &#x27;svm_waimai11.pkl&#x27;)    # f1    f1 = f1_score(pre_test_label,test_label,average=&#x27;micro&#x27;)    print(f&quot;f1:&#123;f1&#125;&quot;)    # Confusion Matrix    metrics_out = confusion_matrix(test_label, pre_test_label)      print(f&quot;TP: &#123;metrics_out[0][0]&#125;&quot;)    print(f&quot;FN: &#123;metrics_out[0][1]&#125;&quot;)    print(f&quot;FP: &#123;metrics_out[1][0]&#125;&quot;)    print(f&quot;TN: &#123;metrics_out[1][1]&#125;&quot;)\n2. Conclusion3. Attachmentüìé(Dataset)Dataset\n"},{"title":"(NLP) Sentiment classification using LSTMs and torchtext.","url":"/2022/12/13/%5BNLP%5D%20Sentiment%20classification%20using%20LSTMs%20and%20torchtext/","content":"0. Statement. üè´Today I want to use LSTM to do sentiment classification on the IMDB dataset. üòá\n1. Experimental field. üòäI am deploying today‚Äôs experiment on colab because of the huge amount of LSTM operations.üò≠\n2. Data pre-processing in NLP. ü§î\nImport of text data.\nDivision of the text data set (training set, validation set, and testing set).\nWord separation.\nConstruction of vocabulary.\nEncoding and mapping of text data to vocabularies.\nGeneration of word vectors.\nGeneration of batch text data.\n\n3. Intro torchtext. üßêRecommend this blog about torchtext.\n4. Pre-processing of IMDB datasets.‚ö†Ô∏è: Filed -&gt; splits -&gt; build_vocab\nimport numpy as npimport torchfrom torch import nn, optim!pip install torch==1.8.0 torchtext==0.9.0from torchtext.legacy import data, datasetstorch.manual_seed(1024)!python -m spacy download en_core_web_md TEXT = data.Field(tokenize=&#x27;spacy&#x27;, tokenizer_language=&#x27;en_core_web_md&#x27;)LABEL = data.LabelField(dtype=torch.float)train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)print(&#x27;len of train data:&#x27;, len(train_data))        print(&#x27;len of test data:&#x27;, len(test_data))          print(train_data.examples[10].text)print(train_data.examples[10].label)TEXT.build_vocab(train_data, max_size=10000, vectors=&#x27;glove.6B.100d&#x27;)LABEL.build_vocab(train_data)print(len(TEXT.vocab))                     print(TEXT.vocab.itos[:])                  print(TEXT.vocab.stoi[&#x27;here&#x27;])              print(LABEL.vocab.stoi)      batchsz = 30train_iterator, test_iterator = data.BucketIterator.splits(                                (train_data, test_data),                                batch_size = batchsz,                               )              \n\n5. Construction of neural network.class lstm(nn.Module):    def __init__(self, vocab_size, embedding_dim, hidden_dim):        super(lstm, self).__init__()        self.embedding = nn.Embedding(vocab_size, embedding_dim)                self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,                                bidirectional=True, dropout=0.5)        self.fc = nn.Linear(hidden_dim*2, 1)        self.dropout = nn.Dropout(0.5)    def forward(self, x):            embedding = self.dropout(self.embedding(x))        output, (hidden, cell) = self.rnn(embedding)                  hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)           hidden = self.dropout(hidden)        out = self.fc(hidden)        return outlstm = lstm(len(TEXT.vocab), 100, 256)print(lstm)\n\n6. Updating the embedding layer with Glove‚Äôs parameters.pretrained_embedding = TEXT.vocab.vectorsprint(&#x27;pretrained_embedding:&#x27;, pretrained_embedding.shape)    lstm.embedding.weight.data.copy_(pretrained_embedding)print(&#x27;embedding layer inited.&#x27;)\n\n7. Optimizer &amp; criterion.optimizer = optim.Adam(lstm.parameters(), lr=5e-2)criterion = nn.BCEWithLogitsLoss() \n8. Construction of accuracy function.def binary_acc(preds, y):     preds = torch.round(torch.sigmoid(preds))     correct = torch.eq(preds, y).float()     acc = correct.sum() / len(correct)     return acc\n9. Training &amp; testingdef train(lstm, iterator, optimizer, criterion):    avg_acc = []    lstm.train()            for i, batch in enumerate(iterator)            pred = lstm(batch.text).squeeze(1)                    loss = criterion(pred, batch.label)        acc = binary_acc(pred, batch.label).item()           avg_acc.append(acc)        optimizer.zero_grad()        loss.backward()        optimizer.step()                                     print(i, acc)    avg_acc = np.array(avg_acc).mean()    print(&#x27;avg acc:&#x27;, avg_acc)def evaluate(lstm, iterator, criterion):    avg_acc = []    lstm.eval()             with torch.no_grad():        for batch in iterator:            pred = lstm(batch.text).squeeze(1)                  loss = criterion(pred, batch.label)            acc = binary_acc(pred, batch.label).item()            avg_acc.append(acc)    avg_acc = np.array(avg_acc).mean()    print(&#x27;test acc:&#x27;, avg_acc)    for epoch in range(5):  train(lstm, train_iterator, optimizer, criterion)  evaluate(lstm, test_iterator, criterion)\nFinally ü§©Thank you for the current age of knowledge sharing and the people willing to share it, thank you! The knowledge on this blog is what I‚Äôve learned on this site, thanks for the support! üòá\n"}]