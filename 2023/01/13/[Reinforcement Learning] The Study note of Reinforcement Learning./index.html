<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    <meta name="description" content="Hexo Theme Redefine">
    <meta name="author" content="Jun-ho Chae">
    
    <title>
        
            (Reinforcement Learning) The Study note of Reinforcement Learning. |
        
        Jun-ho Chae&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/c-solid.svg">
    
<link rel="stylesheet" href="/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/css/v5-font-face.min.css">

    
<link rel="stylesheet" href="/css/duotone.min.css">

    
<link rel="stylesheet" href="/css/brands.min.css">

    
<link rel="stylesheet" href="/css/solid.min.css">

    
<link rel="stylesheet" href="/css/css2.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <script id="hexo-configurations">
    let REDEFINE = window.REDEFINE || {};
    REDEFINE.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.json"};
    REDEFINE.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#005080","avatar":"/images/c-solid.svg","favicon":"/images/c-solid.svg","article_img_align":"center","right_side_width":"210px","content_max_width":"1000px","nav_color":{"left":"#f78736","right":"#367df7","transparency":35},"hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_image":{"light":"/images/light1111.jpg","dark":"/images/dark111.jpg"},"title_color":{"light":"#fff","dark":"#d1d1b6"},"description":"Welcome to my Blog. Have a nice day! üòä"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":false}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true},"pjax":{"enable":true},"lazyload":{"enable":true},"version":"0.3.5"};
    REDEFINE.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fas fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">
    
    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                Jun-ho Chae&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                HOME
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                ARCHIVES
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               target="_blank" rel="noopener" href="https://blog.csdn.net/cjh0318"
                            >
                                LINKS
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                ABOUT
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">HOME</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">ARCHIVES</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       target="_blank" rel="noopener" href="https://blog.csdn.net/cjh0318">LINKS</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">ABOUT</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">
            <div class="article-title">
                <span class="title-hover-animation"><h1 style="font-size:2rem; font-weight: bold; margin: 10px 0;">(Reinforcement Learning) The Study note of Reinforcement Learning.</h1></span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/c-solid.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Jun-ho Chae</span>
                            
                                <span class="author-label">lol</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-duotone fa-pen-line"></i>&nbsp;
        <span class="pc">2023-01-13 20:44:55</span>
        <span class="mobile">2023-01-13 20:44</span>
    </span>
    
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="0-Statement-ü´°"><a href="#0-Statement-ü´°" class="headerlink" title="0. Statement ü´°"></a>0. Statement ü´°</h1><p>This blog is my self-study study notes on reinforcement learning based on <a class="link"   target="_blank" rel="noopener" href="https://deeplizard.com/learn/video/nyjbcRQ-uQ8" >this website<i class="fas fa-external-link-alt"></i></a>. After that, I organized some of the more core and important contents of the course. üßê</p>
<h1 id="Start-ü§†"><a href="#Start-ü§†" class="headerlink" title="Start ü§†"></a>Start ü§†</h1><p>Reinforcement Learning. - Reinforcement learning algorithms seek to find a policy that will yield more return to the agent than all other policies.</p>
<ol>
<li>A Markov decision - Agent, Environment, State, Action, Reward.<br>The goal of an agent in an MDP is to maximize its cumulative rewards.</li>
<li>The cumulative rewards.<br>The expected return is what‚Äôs driving the agent to make the decisions it makes.</li>
</ol>
<ul>
<li>It is the agent‚Äôs goal to maximize the expected return of rewards.</li>
</ul>
<ol start="3">
<li>Tasks<ol>
<li>Episodic tasks - The agent-environment. interaction naturally breaks up into subsequences. Such as the ping-pong game. </li>
<li>Continuing tasks.<ol>
<li>Discounted Return - Rather than the agent‚Äôs goal being to maximize the expected return of rewards, it will instead be to maximize the expected discounted return of rewards. The agent will be choosing action A-t at each time t to maximize the expected discounted return.</li>
</ol>
</li>
</ol>
</li>
</ol>
<ul>
<li>It is the agent‚Äôs goal to maximize the expected discounted return of rewards.</li>
<li>The discount rate will be the rate for which we discount future rewards and will determine the present value of future rewards. This definition of the discounted return makes it to where our agent will care more about the immediate reward over future rewards since future rewards will be more heavily discounted.</li>
<li>The infinite sum of discounted returns is finite if the conditions as long as the reward are nonzero and constant,  and lambda less than 1.</li>
</ul>
<ol start="4">
<li>Policy(œÄ) - How probable is it for an agent to select any action from a given state?<ol>
<li>Definition: A policy is a function that maps a given state to probabilities of selecting each possible action from that state.</li>
</ol>
</li>
<li>Value functions - How good is any given or any given state for an agent?<ol>
<li>Definition: Value functions are functions of states, or of state-action pairs, that estimate how good it is for an agent to be in a given state, or how good it is for the agent to perform a  given action in a given state.</li>
</ol>
</li>
<li>State-Value  Function(VœÄ) VœÄ(s)<ol>
<li>The state-value function for policy œÄ, denoted as VœÄ, tells us how good any given state is for an agent following policy œÄ. (It gives us the value of a state under œÄ)</li>
</ol>
</li>
<li>Action-Value Function(qœÄ) qœÄ(s, a)<ol>
<li>The action-value function for policy œÄ,  denoted as qœÄ, tells us how good it is for the agent to take any given action from a given state while following policy œÄ. (It gives us the value of action under œÄ)</li>
<li>The action-value function qœÄ is referred to as the Q-function, and the output from the function for any given state-action pair is called a Q-value. (The letter ‚ÄúQ‚Äù is used to represent the quality of taking a given action in a given state.)</li>
</ol>
</li>
<li>Wrapping up<ol>
<li>The state-value function tells us how good any given state is for the agent, whereas the action-value function tells us how good it is for the agent to take any action from a given state.</li>
<li>Value functions are defined with respect to the expected return, specific ways of acting, and the policy.</li>
</ol>
</li>
<li>Optimal policies. - A policy that is better than or at least the same as all other policies.</li>
<li>Optimal State-value Function V*: V* gives the largest expected return achievable by any policy œÄ for each state.</li>
<li>Optimal Acton-Value Function q*: q* gives the largest expected return achievable by any policy œÄ for each possible state-action pair.</li>
<li>Q-learning: It is a reinforcement learning technique used for learning the optimal policy in a Markov Decision Process.<ol>
<li>The goal of Q-learning is to find the optimal policy by learning the optimal Q-values for each state-action pair.</li>
<li>Exploration Vs. Exploitation: <ol>
<li>Exploration is the act of exploring the environment to find out information about it.</li>
<li>Exploitation is the act of exploiting the information that is already known about the environment in order to maximize the return.</li>
<li>To get this balance between exploitation and exploration, we use what is called an epsilon greedy strategy.</li>
</ol>
</li>
</ol>
</li>
<li>Choosing Actions with An Epsilon Greedy Strategy.<ol>
<li>With this strategy, we define an exploration rate epsilon that we initially set to 1. This exploration rate is the probability that our agent will explore the environment rather than exploit it. With epsilon &#x3D; 1, it is 100% certain that the agent will start out by exploring the environment.<br>As the agent learns more about the environment, at the start of each new episode, epsilon will decay by some rate that we set so that the likelihood of exploration becomes less and less probable as the agent learns more and more about the environment. </li>
<li>We generate a random number between 0 and 1. If this number is greater than epsilon, then the agent will choose its next action via exploitation, i.e. it will choose the action with the highest Q-value for its current state from the Q-table. Otherwise, its action will be chosen via exploration, i.e. randomly choosing its action and exploring what happens in the environments.</li>
</ol>
</li>
<li>All of the steps for Q-learning.<ol>
<li>Initialize all Q-values in the Q-table to 0.</li>
<li>For each time-step in each episode:<ol>
<li>Choose an action (Considering the exploration-exploitation trade-off).</li>
<li>Observe the reward and next state.</li>
<li>Update the Q-value function(Make the Q-value function converge to the right hand side of the Bellman equation).</li>
</ol>
</li>
</ol>
</li>
</ol>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;FrozenLake-v1&#x27;</span>, render_mode=<span class="string">&#x27;ansi&#x27;</span>)</span><br><span class="line"></span><br><span class="line">action_space_size = env.action_space.n</span><br><span class="line">state_space_size = env.observation_space.n</span><br><span class="line"></span><br><span class="line">q_table = np.zeros(</span><br><span class="line">    (state_space_size, action_space_size)</span><br><span class="line">)</span><br><span class="line">a = np.zeros(</span><br><span class="line">    (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(q_table)</span><br><span class="line"></span><br><span class="line">num_episodes = <span class="number">1000</span></span><br><span class="line">max_steps_per_episode = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">discount_rate = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line">exploration_rate = <span class="number">1</span></span><br><span class="line">max_exploration_rate = <span class="number">1</span></span><br><span class="line">min_exploration_rate = <span class="number">0.01</span></span><br><span class="line">exploration_decay_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">rewards_all_episodes = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Q-learning algorithm</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">    state = env.reset()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    rewards_current_episode = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_steps_per_episode):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Exploration-exploitation trade-off</span></span><br><span class="line">        exploration_rate_threshold = random.uniform(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> exploration_rate_threshold &gt; exploration_rate:</span><br><span class="line">            action = np.argmax(q_table[state,:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = env.action_space.sample()</span><br><span class="line"></span><br><span class="line">        new_state, reward, done, truncated, info = env.step(action)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update Q-table for Q(s, a)</span></span><br><span class="line">        q_table[state, action] = q_table[state, action] * (<span class="number">1</span> - learning_rate) + \</span><br><span class="line">            learning_rate * (reward + discount_rate * np.<span class="built_in">max</span>(q_table[new_state, :]))</span><br><span class="line"></span><br><span class="line">        state = new_state</span><br><span class="line">        rewards_current_episode += reward</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> done == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Exploration rate decay</span></span><br><span class="line">    exploration_rate = min_exploration_rate + \</span><br><span class="line">                       (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)</span><br><span class="line"></span><br><span class="line">    rewards_all_episodes.append(rewards_current_episode)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate and print the average reward per thousand episodes</span></span><br><span class="line">rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / <span class="number">1000</span>)</span><br><span class="line">count = <span class="number">1000</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======== Average reward per thousand episodes========\n&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> rewards_per_thousand_episodes:</span><br><span class="line">    <span class="built_in">print</span>(count, <span class="string">&quot;: &quot;</span>, <span class="built_in">str</span>(<span class="built_in">sum</span>(r / <span class="number">1000</span>)))</span><br><span class="line">    count += <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print updated Q-table</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n\n========Q-table========\n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(q_table)</span><br></pre></td></tr></table></figure></div>


<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/32dab278ef8b47768241dd078439b626.png"
                      alt="ËØ∑Ê∑ªÂä†ÂõæÁâáÊèèËø∞"
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://img-blog.csdnimg.cn/23dbc323e8804fb68396511bdb5db143.png"
                      alt="ËØ∑Ê∑ªÂä†ÂõæÁâáÊèèËø∞"
                ></p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li>Post titleÔºö(Reinforcement Learning) The Study note of Reinforcement Learning.</li>
        <li>Post authorÔºöJun-ho Chae</li>
        <li>Create timeÔºö2023-01-13 20:44:55</li>
        <li>
            Post linkÔºöhttps://redefine.evanluo.top/2023/01/13/[Reinforcement Learning] The Study note of Reinforcement Learning./
        </li>
        <li>
            Copyright NoticeÔºöAll articles in this blog are licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> unless stating additionally.
        </li>
    </ul>
</div>

                </div>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2023/01/14/%5BAlgorithm%5D%20How%20should%20we%20think%20about%20recursion_%20(Contains%20two%20algorithm%20questions)/"
                            >
                                <span class="left arrow-icon flex-center">
                                <i class="fas fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">(Algorithm) How should we think about recursion? (Contains two algorithm questions)</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2022/12/13/%5BNLP%5D%20Summary%20of%20the%20input%20details%20of%20embedding%20layer,%20LSTM%20layer./"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">(NLP) Summary of the input details of embedding layer, LSTM layer.</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                <i class="fas fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments">&nbsp;Comments</i>
    </div>
    

        
            

        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div style="font-size: 1.3rem;margin-top: 0; margin-bottom: 0.8rem; transition-duration: 0.1s;"><i class="fa-solid fa-list"></i> <strong>Contents</strong></div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-Statement-%F0%9F%AB%A1"><span class="nav-text">0. Statement ü´°</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Start-%F0%9F%A4%A0"><span class="nav-text">Start ü§†</span></a></li></ol>
    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>



        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2020</span>
              -
            
            2023&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">Jun-ho Chae</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv">
                        Visitor Count&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                    </span>
                
                
                    <span id="busuanzi_container_site_pv">
                        Totalview&nbsp;<span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            Powered by <a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;|&nbsp;Theme&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v0.3.5</a>
        </div>
        
        
    </div>
    <link rel="stylesheet" href="//evan.beee.top/css/waline.css"/>
    <script src="//evan.beee.top/js/waline.js"></script>
    
<link rel="stylesheet" href="/css/regular.min.css">

</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fa-duotone fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-duotone fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>




    
<script src="/js/lazyload.js"></script>



<div class="post-scripts pjax">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            REDEFINE.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            REDEFINE.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            REDEFINE.refresh();
        });
    });
</script>



</body>
</html>
